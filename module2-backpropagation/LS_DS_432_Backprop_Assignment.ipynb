{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_Backprop_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "U4S2",
      "language": "python",
      "name": "u4s2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Backpropagation Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Using TensorFlow Keras, Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn.\n",
        "\n",
        "This is your \"Hello World!\" of TensorFlow.\n",
        "\n",
        "### Example TensorFlow Starter Code\n",
        "\n",
        "```python \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(3, activation='sigmoid', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=100)\n",
        "\n",
        "```\n",
        "\n",
        "### Additional Written Tasks:\n",
        "1. Investigate the various [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which is best suited for the task at hand (predicting 1 / 0) and why? \n",
        "\n",
        "**The loss function most ideal for the task at hand would be BinaryCrossentropy, because it compares the true values with the predicted values (as the name of the loss function suggests, it's for True / False (binary) classification issues).**\n",
        "\n",
        "2. What is the difference between a loss function and a metric? Why might we need both in Keras? \n",
        "\n",
        "**A loss function is meant for optimization of the model, whereas the metric is meant for evaluating the model's performance. Both are needed because the loss function's purpose is to optimize the model, and the metric is meant to determine the quality of this optimization in terms of effect on model performance.**\n",
        "\n",
        "3. Investigate the various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Stochastic Gradient Descent (`sgd`) is not the learning algorithm dejour anyone. Why is that? What do newer optimizers such as `adam` have to offer? \n",
        "\n",
        "**Stochastic Gradient Descent is not the learning algorithm de jour anymore because when working with larger amounts of data, you're prone to more noise. Because of this, on each epoch, the learning step can go back and forth, wandering around the minimum without ever actually converging into an acceptable solution. SDG, due to the nature of the algorithm, also is computationally expensive, because it performs the update of parameters on each example. Newer optimizers such as Adam are more efficient in terms of computation and memory requirements. According to TensorFlow documentation, they're also invatiant to rescaling of gradients diagonally, and more suited for problems that are large in terms of size of data and/or parameters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ukk-3RkA7smp"
      },
      "source": [
        "### Build a Tensor Keras Perceptron\n",
        "\n",
        "Try to match the architecture we used on Monday - inputs nodes and one output node. Apply this architecture to the XOR-ish dataset above. \n",
        "\n",
        "After fitting your model answer these questions: \n",
        "\n",
        "Are you able to achieve the same results as a bigger architecture from the first part of the assignment? Why is this disparity the case? What properties of the XOR dataset would cause this disparity? \n",
        "\n",
        "Now extrapolate this behavior on a much larger dataset in terms of features. What kind of architecture decisions could we make to avoid the problems the XOR dataset presents at scale? \n",
        "\n",
        "*Note:* The bias term is baked in by default in the Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WsM-mywtC2Ir",
        "outputId": "37e56b5b-ccc3-476c-a2d7-d67e95862e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'x1':[0, 0, 1, 0 , 1, 1, 0],\n",
        "                   'x2':[0, 1, 0, 1, 0, 1, 0],\n",
        "                   'x3':[1, 1, 1, 0, 0, 1, 0],\n",
        "                   'y':[0, 1, 1, 1, 1, 0, 0]\n",
        "                  })\n",
        "\n",
        "feats = list(df)[:-1]  # Extracting the x features columns\n",
        "features = df[feats].values  # Getting the values from the features columns\n",
        "target = df['y'].values  # This is our target we're trying to predict\n",
        "\n",
        "print('Features (X values):', feats)  # Making sure features are correct.\n",
        "print(\"Values to predict (y value):\", target)  # Making sure the target we're attempting to predict is correct.\n",
        "\n",
        "print(f'Shape of Features: {features.shape}')  # Confirming shape of features to pass into the input dimensions for our input layer."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features (X values): ['x1', 'x2', 'x3']\n",
            "Values to predict (y value): [0 1 1 1 1 0 0]\n",
            "Shape of Features: (7, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OzZQU-O3nC32"
      },
      "source": [
        "#### Building Keras Perceptron (With Hidden Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEREYT-3wI1f",
        "outputId": "b251cd9f-d259-4a9c-d505-ba22f63aa1c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "# Keras Imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model1 = Sequential([\n",
        "                    Dense(3, input_dim=3),  # We do not apply the activation function to this because it's the input layer, correct?\n",
        "                    Dense(4, activation='sigmoid'),  # This is our hidden layer with four nodes.\n",
        "                    Dense(1, activation='sigmoid')  # This will be our output layer. Because this is binary classification, only 1 output is needed.\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='RMSProp', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "results = model1.fit(features, target, epochs=25)  # Fitting our model (100 iterations / epochs)\n",
        "score1 = model1.evaluate(features, target)  # Note: This will spit out both our loss and our metric (accuracy here)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8141 - accuracy: 0.4286\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8110 - accuracy: 0.4286\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8088 - accuracy: 0.4286\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8069 - accuracy: 0.4286\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8053 - accuracy: 0.4286\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8039 - accuracy: 0.4286\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8025 - accuracy: 0.4286\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8012 - accuracy: 0.4286\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8000 - accuracy: 0.4286\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7988 - accuracy: 0.4286\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7977 - accuracy: 0.4286\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7966 - accuracy: 0.4286\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7956 - accuracy: 0.4286\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7945 - accuracy: 0.4286\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7935 - accuracy: 0.4286\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7926 - accuracy: 0.4286\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7916 - accuracy: 0.4286\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7907 - accuracy: 0.4286\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7897 - accuracy: 0.4286\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7888 - accuracy: 0.4286\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7879 - accuracy: 0.4286\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7870 - accuracy: 0.4286\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7861 - accuracy: 0.4286\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7852 - accuracy: 0.4286\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7844 - accuracy: 0.4286\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7835 - accuracy: 0.4286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fm5ti2c7ldsw",
        "outputId": "65839d39-d579-4572-bf5a-5e1009cd18b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print('Model Evaluation:')\n",
        "print(f'Metric is {model1.metrics_names[1]}: {round(score1[1], 3)}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Evaluation:\n",
            "Metric is accuracy: 0.429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CNn8MuTurjrc"
      },
      "source": [
        "#### Building Keras Perceptron (Without Hidden Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "klhaPUx3rn1U",
        "outputId": "4a532d80-d38f-43f6-84f8-d03891ba056d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "model2 = Sequential([\n",
        "                    Dense(3, input_dim=3),  # Input layer; same question applies.\n",
        "                    Dense(1, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='RMSProp', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "results = model2.fit(features, target, epochs=25)  # Fitting our model (25 iterations / epochs again)\n",
        "score2 = model2.evaluate(features, target)  # Note: This will spit out both our loss and our metric (accuracy here)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7459 - accuracy: 0.4286\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7446 - accuracy: 0.2857\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7436 - accuracy: 0.2857\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7429 - accuracy: 0.2857\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7422 - accuracy: 0.2857\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7416 - accuracy: 0.2857\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7410 - accuracy: 0.2857\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7405 - accuracy: 0.2857\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7400 - accuracy: 0.2857\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7395 - accuracy: 0.2857\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7390 - accuracy: 0.2857\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7386 - accuracy: 0.2857\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7382 - accuracy: 0.2857\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7377 - accuracy: 0.2857\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7373 - accuracy: 0.2857\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7369 - accuracy: 0.2857\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7365 - accuracy: 0.2857\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7361 - accuracy: 0.2857\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7357 - accuracy: 0.2857\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7354 - accuracy: 0.2857\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 0s 909us/step - loss: 0.7350 - accuracy: 0.2857\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7346 - accuracy: 0.2857\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 0s 946us/step - loss: 0.7343 - accuracy: 0.2857\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7339 - accuracy: 0.2857\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7335 - accuracy: 0.2857\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7332 - accuracy: 0.2857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7FEbEEdCsuzj",
        "outputId": "b12ee75f-31c6-41c8-cfa5-8839fbc7edce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print('Model Evaluation:')\n",
        "print(f'Metric is {model2.metrics_names[1]}: {round(score2[1], 3)}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Evaluation:\n",
            "Metric is accuracy: 0.286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2Z4McKEtT__"
      },
      "source": [
        "#### Evaluations For Both Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9uVRIjaxtXvg",
        "outputId": "fa301743-1f85-4baf-ba87-3a6e9c3b0428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(f'Metric is {model2.metrics_names[1]}: {round(score2[1], 3)} (Without Hidden Layer - Basic Perceptron)')\n",
        "print(f'Metric is {model1.metrics_names[1]}: {round(score1[1], 3)} (With Hidden Layer - Multi-Class Perceptron)')\n",
        "\n",
        "if score2[1] > score1[1]:\n",
        "  print('(Basic Perceptron Outperformed Multi-Class Perceptron)')\n",
        "elif score2[1] == score1[1]:\n",
        "  print('(Both Models Performed Equally)')\n",
        "else:\n",
        "  print('(Multi-Class Perceptron Outperformed Basic Perceptron)')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metric is accuracy: 0.286 (Without Hidden Layer - Basic Perceptron)\n",
            "Metric is accuracy: 0.429 (With Hidden Layer - Multi-Class Perceptron)\n",
            "(Multi-Class Perceptron Outperformed Basic Perceptron)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u57EC_5xm4fW"
      },
      "source": [
        "#### Answered Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8q4PgaMWnNOT"
      },
      "source": [
        "Are you able to achieve the same results as a bigger architecture from the first part of the assignment? Why is this disparity the case? What properties of the XOR dataset would cause this disparity?\n",
        "\n",
        "**By adding a hidden layer, I was able to achieve better results. This is because of the nature of XOR gates. XOR stands for exclusive or, meaning that the true output is only true if only *one* of the inputs to the gate is true. This is related to linear seperability, and for a long time was a common criticism of perceptrons / neural networks as a whole. This problem was solved with the implementation of multi-layered perceptrons.**\n",
        "\n",
        "Now extrapolate this behavior on a much larger dataset in terms of features. What kind of architecture decisions could we make to avoid the problems the XOR dataset presents at scale?\n",
        "\n",
        "**Again, XOR gates implement an exclusive or (hence the name), meaning that a true output results if one (ONLY one) of the inputs to the gate is true. This can be solved by having multiple layers. Thus, the primary architecture decisions would be the number of hidden layers, as well as the number of nodes within those layers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use TensorFlow Keras & the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the canonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the Internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
        "\n",
        "\n",
        "### Parts\n",
        "1. Gathering & Transforming the Data\n",
        "2. Making MNIST a Binary Problem\n",
        "3. Estimating your Neural Network (the part you focus on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ey8zcb2i7sm6"
      },
      "source": [
        "### Gathering the Data \n",
        "\n",
        "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1j7GP6TG7sm7",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvb7XHMo7snE",
        "colab": {}
      },
      "source": [
        "# Input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umgju9Wk7snN",
        "colab": {}
      },
      "source": [
        "# Load Data & Train-Test-Split\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rk2-SMoq7snY",
        "colab": {}
      },
      "source": [
        "# Reshaping data in accordance to image dimensions\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols)\n",
        "\n",
        "# Normalize Our Data\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CpiTx0EQ7snh",
        "outputId": "862b0b51-34e5-4e19-90f0-a62dde4166c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Now the data should be in a format you're more familiar with\n",
        "X_train.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GGD_fnZA7snu"
      },
      "source": [
        "### Making MNIST a Binary Problem \n",
        "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simplify the problem for now: Zero or all else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XIfqhU_f7snw",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_temp = np.zeros(y_train.shape)\n",
        "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
        "y_train = y_temp\n",
        "\n",
        "y_temp = np.zeros(y_test.shape)\n",
        "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
        "y_test = y_temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h7nObosn7sn8",
        "outputId": "e64739e4-04b8-47dd-e4d1-ad4aadc9275b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Binary target to work with.\n",
        "y_train"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yaHvxlNS7soF"
      },
      "source": [
        "### Estimating Your Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MOPtYdk1HgA",
        "outputId": "c3dc5122-9db2-4fa2-9b41-85fab26bc00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "model3 = Sequential([\n",
        "                     Dense(125, input_dim=784),  # What is generally considered the best way to determine number of nodes?\n",
        "                     Dense(10, activation='sigmoid'),  # Hidden layer number 1.\n",
        "                     Dense(5, activation='sigmoid'),  # Hidden layer number 2\n",
        "                     Dense(1, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Compile\n",
        "\n",
        "model3.fit(X_train, y_train)\n",
        "score3 = model3.evaluate(X_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2233 - accuracy: 0.9371\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AXEHG8DCx5be",
        "outputId": "e4ac0eb5-462b-4aa5-9335-0abb906df11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print('For MNIST Dataset Neural Network:')\n",
        "print(f'Metric ({model3.metrics_names[1]}): {score3[1] * 100}%')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For MNIST Dataset Neural Network:\n",
            "Metric (accuracy): 99.26999807357788%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}