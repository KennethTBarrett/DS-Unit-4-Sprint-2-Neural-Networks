{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_421_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "The input layer is the part of our data that the neural network is exposed to and interacts with. Each input node is typically assigned to one feature.\n",
        "\n",
        "### Hidden Layer:\n",
        "The magic of modern neural networks; any layer between input and output layer. The data can not be accessed or directly interact with in this layer. It is passed through obscured to the output.\n",
        "\n",
        "### Output Layer:\n",
        "The output layer puts vector values suitable to the type of problem we're trying to assess (regression, classification, etc). They have a node for each type of output (for example, binary classification will have a single output node (probability)).\n",
        "\n",
        "### Neuron:\n",
        "Neurons / Nodes receive inputs and pass on the signal to the next layer of nodes if a certain threshold is met.\n",
        "\n",
        "### Weight:\n",
        "The weight gives us the range of motion to move up and down the equation. Each step along the way, each layer affects the next layer by using the weighted sum of our inputs (plus bias). One way to think of weights is the strength of connection between nodes.\n",
        "\n",
        "### Activation Function:\n",
        "Activation functions determine how much signal is sent from the previous layer to the next. Hidden and output layers may contain activation functions, while input layers don't.\n",
        "\n",
        "### Node Map:\n",
        "A node map is a visual diagram that serves as a representation of the neural network's architecture. They are used because the equations to represent the neural network are so complicated that it would be painstakingly hard to read and interpret what is going on within them.\n",
        "\n",
        "### Perceptron:\n",
        "A perceptron is the simplest version of a neural network. It's a single node with nothing else but the input. It will take the inputs, calculate with weight, sum up all predictions, then pass the sum to the activation function. The result of this will be our final value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "The neural network takes the input layers, and multiples the weight and add the bias to the inputs. These are then sent to the activation function, which determines how much signal is passed on to the next layer. This is repeated through the hidden layers until the output layer is reached, at which point there will be a node for each type of output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RrTEJTi8ujU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "14b8fb44-7741-4640-b198-1812f3587d40"
      },
      "source": [
        "# NOTE TO BRANDON: I'm pretty sure I got this part down. It's the next part that killed Jacob & I.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')\n",
        "df['bias'] = [1,1,1,1]  # Adding column for bias.\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   x1  x2  y  bias\n",
              "0   0   0  1     1\n",
              "1   1   0  1     1\n",
              "2   0   1  1     1\n",
              "3   1   1  0     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7-EGhgWFl8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_outputs = [[0], [0], [1], [1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {}
      },
      "source": [
        "# Sigmoid activation function + derivative for updating weights\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sx = sigmoid(x)\n",
        "    return sx * (1 - sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n56nk3N_BKl",
        "colab_type": "text"
      },
      "source": [
        "### Split Up Individually For Comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsK_0IUM8umF",
        "colab_type": "code",
        "outputId": "34247df7-b1b6-41b3-fcd8-2bdb89d1e436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# First, we need to initialize random weights for our inputs (3 total)\n",
        "weights = 2 * np.random.random((4,1)) - 1\n",
        "print('Randomized Weights: \\n', weights)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Randomized Weights: \n",
            " [[ 0.27587198]\n",
            " [-0.44556584]\n",
            " [ 0.18074124]\n",
            " [-0.03173615]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuHqqvQ98unQ",
        "colab_type": "code",
        "outputId": "6ab93544-848c-4a7b-bfdd-6b603e0bf3b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Now, let's calculate the weighted sums of inputs and weights.\n",
        "weighted_sums = np.dot(df, weights)\n",
        "print('Weighted Sums: \\n', weighted_sums)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weighted Sums: \n",
            " [[ 0.1490051 ]\n",
            " [ 0.42487708]\n",
            " [-0.29656074]\n",
            " [-0.20143001]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t6sayyi8unr",
        "colab_type": "code",
        "outputId": "cf7539d0-0ba1-4d83-ea1d-68ecfab7f2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Now we need to output the activated value at the end of 1 training epoch.\n",
        "activated_outputs = sigmoid(weighted_sums)\n",
        "print('Predicted Probability: \\n', activated_outputs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Probability: \n",
            " [[0.5371825 ]\n",
            " [0.6046497 ]\n",
            " [0.42639845]\n",
            " [0.44981208]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUTfy_DA8uoR",
        "colab_type": "code",
        "outputId": "cab2f248-5f77-4d67-ce55-267b358a4be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Now, we need to calculate our error.\n",
        "\n",
        "error = correct_outputs - activated_outputs\n",
        "print('Error: \\n', error)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: \n",
            " [[-0.5371825 ]\n",
            " [-0.6046497 ]\n",
            " [ 0.57360155]\n",
            " [ 0.55018792]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCMGToR-8uot",
        "colab_type": "code",
        "outputId": "180ebc7d-0853-495b-fb7a-fe880d2f10dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Make adjustments through backprop\n",
        "adjustments = error + sigmoid_derivative(weighted_sums)\n",
        "print('Adjustments: \\n', adjustments)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjustments: \n",
            " [[-0.28856504]\n",
            " [-0.36560126]\n",
            " [ 0.81818436]\n",
            " [ 0.7976691 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e4FKlIl8upD",
        "colab_type": "code",
        "outputId": "9e6a9d62-ac81-43a5-88ea-abb407cab654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Update weights based upon gradient descent.\n",
        "weights += np.dot(df.T, adjustments)\n",
        "print(weights)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.70793981]\n",
            " [1.17028762]\n",
            " [0.3447593 ]\n",
            " [0.929951  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml1MpUnD_G7D",
        "colab_type": "text"
      },
      "source": [
        "### For Loop to Put it All Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iqoxkq1GHxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "01fb7a83-0c0f-46a2-ae5b-89868855a981"
      },
      "source": [
        "for iteration in range(10000):\n",
        "    \n",
        "    # Weighted sum of inputs / weights\n",
        "    weighted_sum = np.dot(df, weights)\n",
        "    \n",
        "    # Activate\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    \n",
        "    # Calculate error\n",
        "    error = correct_outputs - activated_output\n",
        "    \n",
        "    # Calculate adjustements\n",
        "    adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "    \n",
        "    # Update the Weights\n",
        "    weights += np.dot(df.T, adjustments)\n",
        "\n",
        "print(f'Predicted Values: \\n{activated_output}')\n",
        "print(f'Weights: \\n{weights}')\n",
        "print(f'Error: \\n{error}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Values: \n",
            "[[0.01070925]\n",
            " [0.00333743]\n",
            " [0.99003923]\n",
            " [0.99840758]]\n",
            "Weights: \n",
            "[[-1.17333857]\n",
            " [ 9.12507147]\n",
            " [-3.01517505]\n",
            " [-1.5107551 ]]\n",
            "Error: \n",
            "[[-0.01070925]\n",
            " [-0.00333743]\n",
            " [ 0.00996077]\n",
            " [ 0.00159242]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "\n",
        "\n",
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX8JPvhz8upl",
        "colab_type": "code",
        "outputId": "5eaaf7de-59db-4bb2-9022-d2741af495e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# NOTE TO BRANDON: I had hands down the hardest time with this section.\n",
        "# I'd like to talk about this a little bit during 1:1 with you to further comprehension!\n",
        "# I >kinda< got it. Emphasis on >>kinda<<\n",
        "\n",
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnIEQGdC8uqW",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niRum4jX8uqn",
        "colab_type": "code",
        "outputId": "1bbbdddc-dd71-4cac-ceed-33aa0e5b04e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feats = list(diabetes)[:-1]  # List of features (All columns minus the outcome)\n",
        "\n",
        "transformer = Normalizer().fit(diabetes[feats])  # Instantiate / Fit the Normalizer.\n",
        "X = transformer.transform(diabetes[feats])  # Transform using the fit.\n",
        "y = diabetes['Outcome'].values  # Defining our target.\n",
        "\n",
        "# Train-Test-Split (This is needed to properly predict, no?)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(f'''Train Shapes (77%):\n",
        "X (Features): {X_train.shape}\n",
        "y (Target): {y_train.shape}\n",
        "''')\n",
        "\n",
        "print(f'''Test Shapes (33%):\n",
        "X (Features): {X_test.shape}\n",
        "y (Target): {y_test.shape}\n",
        "''')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shapes (77%):\n",
            "X (Features): (514, 8)\n",
            "y (Target): (514,)\n",
            "\n",
            "Test Shapes (33%):\n",
            "X (Features): (254, 8)\n",
            "y (Target): (254,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "class Perceptron:\n",
        "    \n",
        "    def __init__(self, niter = 100):  # Constructor\n",
        "        self.niter = niter\n",
        "    \n",
        "    def __sigmoid(self, x):  # Sigmoid Activation function\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def __sigmoid_derivative(self, x):  # Sigmoid Derivative function\n",
        "        sx = sigmoid(x)\n",
        "        return sx * (1 - sx)\n",
        "\n",
        "    def fit(self, X, y):  # This will be our fit method.\n",
        "      \"\"\"Fit training data\n",
        "      X : Training vectors, X.shape : [#samples, #features]\n",
        "      y : Target values, y.shape : [#samples]\n",
        "      \"\"\"\n",
        "\n",
        "        # Randomly Initialize Weights\n",
        "      self.weights = 2 * np.random.random((8,1)) - 1  # Using self because the weights will be needed in the predict method.\n",
        "\n",
        "      for i in range(self.niter):\n",
        "          # Weighted sum of inputs / weights using dot product.\n",
        "            weighted_sum = np.dot(X, self.weights)\n",
        "          # Activate function.\n",
        "            activated_output = self.__sigmoid(weighted_sum)\n",
        "          # Calculate error.\n",
        "            error = y - activated_output\n",
        "          # Make adjustments with sigmoid derivative function.\n",
        "            adjustments = error * self.__sigmoid_derivative(weighted_sum)\n",
        "          # Update the weights in accordance to adjustments.\n",
        "            self.weights = self.weights + np.dot(X.T, adjustments)\n",
        "\n",
        "      return self\n",
        "\n",
        "    def predict(self, X):\n",
        "          \"\"\"Return class label after unit step\"\"\"\n",
        "          weighted_sum = np.dot(X, self.weights)  # We're using the weighted sum again...\n",
        "          activated_output = self.__sigmoid(weighted_sum)  #... to make predictions.\n",
        "          return np.round(activated_output)  # Note to self: must use numpy to round ndarray - returns predicted values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSIXCoAFtPHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pn = Perceptron()\n",
        "pn.fit(X_train, y_train)\n",
        "y_pred = pn.predict(X_test)  # DISCUSS WITH BRANDON: But how would I go about measuring accuracy?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}