{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_421_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "The input layer is the *only* part of our data that the neural network is exposed to and interacts with. Each input node is typically assigned to one feature. It is the first layer within the neural network.\n",
        "\n",
        "### Hidden Layer:\n",
        "The magic of modern neural networks; any layer between input and output layer. The data can not be accessed or directly interact with in this layer. It is passed through obscured to the output, which may take the form of another hidden layer, or alternatively, the output layer.\n",
        "\n",
        "### Output Layer:\n",
        "The output layer puts vector values suitable to the type of problem we're trying to assess (regression, classification, etc). They have a node for each type of output (for example, binary classification will have a single output node (probability)). This can be thought of as the layer that, as the name implies, outputs the results from the neural network.\n",
        "\n",
        "### Neuron:\n",
        "Neurons / Nodes receive inputs and pass on the signal to the next layer of nodes if a certain threshold is met. Each layer contains a specified number of nodes.\n",
        "\n",
        "### Weight:\n",
        "The weight gives us the range of motion to move up and down the equation. Each step along the way, each layer affects the next layer by using the weighted sum of our inputs (plus bias). One way to think of weights is the strength of connection between nodes.\n",
        "\n",
        "### Activation Function:\n",
        "Activation functions determine how much signal is sent from the previous layer to the next. Activation functions are critical to the neural network's performance and are one of the countless parameters that can be tuned using a GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "### Node Map:\n",
        "A node map is a visual diagram that serves as a representation of the neural network's architecture. They are used because the equations to represent the neural network are so complicated that it would be painstakingly hard, if not impossible, and most *certainly* time-consuming to read and interpret what is going on within them.\n",
        "\n",
        "### Perceptron:\n",
        "A perceptron is the simplest version of a neural network. It's a single node with nothing else but the input. It will take the inputs, calculate with weight, sum up all predictions, then pass the sum to the activation function. The result of this will be our final value. There are also multi-layered perceptrons which are actually what solved the neural network issue of linear seperability (refer to [XOR Gate](https://en.wikipedia.org/wiki/XOR_gate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "The neural network takes the input layers, and multiples the weight and add the bias to the inputs. These are then sent to the activation function, which determines how much signal is passed on to the next layer. This is repeated through the hidden layers until the output layer is reached, at which point there will be a node for each type of output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RrTEJTi8ujU",
        "colab_type": "code",
        "outputId": "1113903c-53f8-4075-985c-cd44dd778489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')\n",
        "df['bias'] = [1,1,1,1]  # Adding column for bias.\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   x1  x2  y  bias\n",
              "0   0   0  1     1\n",
              "1   1   0  1     1\n",
              "2   0   1  1     1\n",
              "3   1   1  0     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7-EGhgWFl8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_outputs = [[0], [0], [1], [1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {}
      },
      "source": [
        "# Sigmoid activation function + derivative for updating weights\n",
        "\n",
        "def sigmoid(x):\n",
        "  '''Base sigmoid activation function'''\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  '''Sigmoid derivative activation function'''\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1 - sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n56nk3N_BKl",
        "colab_type": "text"
      },
      "source": [
        "### Split Up Individually For Comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsK_0IUM8umF",
        "colab_type": "code",
        "outputId": "59ea851a-9597-4e59-82ca-8e1ae3b07b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# First, we need to initialize random weights for our inputs (3 total)\n",
        "weights = 2 * np.random.random((4,1)) - 1\n",
        "print('Randomized Weights: \\n', weights)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Randomized Weights: \n",
            " [[-0.12555538]\n",
            " [-0.69626632]\n",
            " [-0.94980346]\n",
            " [ 0.24802523]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuHqqvQ98unQ",
        "colab_type": "code",
        "outputId": "f9ccc477-bf6c-404b-e34a-dffe8b5b6ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# Now, let's calculate the weighted sums of inputs and weights.\n",
        "\n",
        "weighted_sums = np.dot(df, weights)  # Dot product of our dataframe and our weights will give us our weighted sums\n",
        "print('Weighted Sums: \\n', weighted_sums)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weighted Sums: \n",
            " [[-0.70177823]\n",
            " [-0.82733361]\n",
            " [-1.39804455]\n",
            " [-0.57379647]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t6sayyi8unr",
        "colab_type": "code",
        "outputId": "2c91af1e-6b41-472f-bf3c-ca41f50aaa37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# Now we need to output the activated value at the end of 1 training epoch. These are our predicted values.\n",
        "\n",
        "activated_outputs = sigmoid(weighted_sums)  # Apply sigmoid function to our weighted sums to attain our activated outputs\n",
        "print('Predicted Probability: \\n', activated_outputs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Probability: \n",
            " [[0.33141809]\n",
            " [0.30420916]\n",
            " [0.1981266 ]\n",
            " [0.36036127]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUTfy_DA8uoR",
        "colab_type": "code",
        "outputId": "7ab8af05-6822-4a4a-d7bc-7414e1df4089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# Now, we need to calculate our error. This can be done by subtracting our predicted outputs by our correct outputs.\n",
        "\n",
        "error = correct_outputs - activated_outputs\n",
        "print('Error: \\n', error)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: \n",
            " [[-0.33141809]\n",
            " [-0.30420916]\n",
            " [ 0.8018734 ]\n",
            " [ 0.63963873]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCMGToR-8uot",
        "colab_type": "code",
        "outputId": "cb3daa79-b013-4758-d600-127843660b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# Make adjustments through backpropagation\n",
        "\n",
        "adjustments = error + sigmoid_derivative(weighted_sums)  # Make adjustments for weight updates.\n",
        "print('Adjustments: \\n', adjustments)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjustments: \n",
            " [[-0.10983795]\n",
            " [-0.09254321]\n",
            " [ 0.96074585]\n",
            " [ 0.87013975]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e4FKlIl8upD",
        "colab_type": "code",
        "outputId": "291ab275-050f-4194-cf59-da125906aadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# Update weights based upon our gradient descent.\n",
        "weights += np.dot(df.T, adjustments)  # Calculating new weights to update model.\n",
        "print(weights)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.65204116]\n",
            " [ 1.13461929]\n",
            " [-0.19143877]\n",
            " [ 1.87652967]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml1MpUnD_G7D",
        "colab_type": "text"
      },
      "source": [
        "### For Loop to Put it All Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iqoxkq1GHxs",
        "colab_type": "code",
        "outputId": "15abfcb9-8212-4036-bad5-4f58bc3cfe86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "for iteration in range(10000):\n",
        "    \n",
        "    # Weighted sum of inputs / weights\n",
        "    weighted_sum = np.dot(df, weights)\n",
        "    \n",
        "    # Activation function\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    \n",
        "    # Calculate error\n",
        "    error = correct_outputs - activated_output\n",
        "    \n",
        "    # Calculate adjustments\n",
        "    adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "    \n",
        "    # Update the weights\n",
        "    weights += np.dot(df.T, adjustments)\n",
        "\n",
        "print(f'Predicted Values: \\n{activated_output}')\n",
        "print(f'Weights: \\n{weights}')\n",
        "print(f'Error: \\n{error}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Values: \n",
            "[[0.01091517]\n",
            " [0.00293897]\n",
            " [0.98992491]\n",
            " [0.99888368]]\n",
            "Weights: \n",
            "[[-1.32013522]\n",
            " [ 9.09429087]\n",
            " [-3.52919175]\n",
            " [-0.97748542]]\n",
            "Error: \n",
            "[[-0.01091517]\n",
            " [-0.00293897]\n",
            " [ 0.01007509]\n",
            " [ 0.00111632]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "\n",
        "\n",
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX8JPvhz8upl",
        "colab_type": "code",
        "outputId": "71ffb067-0caf-46c5-a167-a475292ffaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnIEQGdC8uqW",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niRum4jX8uqn",
        "colab_type": "code",
        "outputId": "2bed42ef-320f-439d-8fa2-25d1360092a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feats = list(diabetes)[:-1]  # List of features (All columns minus the outcome)\n",
        "\n",
        "transformer = Normalizer().fit(diabetes[feats])  # Instantiate / Fit the Normalizer.\n",
        "X = transformer.transform(diabetes[feats])  # Transform using the fit.\n",
        "y = diabetes['Outcome'].values  # Defining our target.\n",
        "\n",
        "# Train-Test-Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(f'''Train Shapes:\n",
        "X (Features): {X_train.shape}\n",
        "y (Target): {y_train.shape}\n",
        "''')\n",
        "\n",
        "print(f'''Test Shapes:\n",
        "X (Features): {X_test.shape}\n",
        "y (Target): {y_test.shape}\n",
        "''')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shapes:\n",
            "X (Features): (514, 8)\n",
            "y (Target): (514,)\n",
            "\n",
            "Test Shapes:\n",
            "X (Features): (254, 8)\n",
            "y (Target): (254,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "class Perceptron:\n",
        "    \n",
        "    def __init__(self, niter=100):  # Constructor\n",
        "        self.niter = niter\n",
        "    \n",
        "    def __sigmoid(self, x):\n",
        "      \"\"\"Sigmoid activation function\"\"\"\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def __sigmoid_derivative(self, x):\n",
        "      \"\"\"Sigmoid Derivative for Backpropagation\"\"\"\n",
        "      sx = sigmoid(x)\n",
        "      return sx * (1 - sx)\n",
        "\n",
        "    def fit(self, X, y):  # This will be our fit method.\n",
        "      \"\"\"Fit training data\n",
        "      X : Training vectors, X.shape : [#samples, #features]\n",
        "      y : Target values, y.shape : [#samples]\n",
        "      \"\"\"\n",
        "\n",
        "        # Randomly Initialize Weights\n",
        "      self.weights = 2 * np.random.random((8,1)) - 1  # Using self because the weights will be needed in the predict method.\n",
        "\n",
        "      for i in range(self.niter):  # Range of the number of iterations.\n",
        "\n",
        "          # Weighted sum of inputs / weights using dot product.\n",
        "            weighted_sum = np.dot(X, self.weights)\n",
        "\n",
        "          # Activate function.\n",
        "            activated_output = self.__sigmoid(weighted_sum)\n",
        "\n",
        "          # Calculate error.\n",
        "            error = y - activated_output\n",
        "\n",
        "          # Make adjustments with sigmoid derivative function.\n",
        "            adjustments = error * self.__sigmoid_derivative(weighted_sum)\n",
        "\n",
        "          # Update the weights in accordance to adjustments.\n",
        "            self.weights = self.weights + np.dot(X.T, adjustments)\n",
        "\n",
        "      return self\n",
        "\n",
        "    def predict(self, X):\n",
        "          \"\"\"Make predictions of class labels\"\"\"\n",
        "          weighted_sum = np.dot(X, self.weights)  # We're using the weighted sum again...\n",
        "          activated_output = self.__sigmoid(weighted_sum)  #... to make predictions.\n",
        "          return np.round(activated_output)  # Note to self: must use numpy to round ndarray - returns predicted values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSIXCoAFtPHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pn = Perceptron()  # Instantiate our perceptron\n",
        "\n",
        "pn.fit(X, y)  # Fit our model\n",
        "y_pred = pn.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nycIPY54Is_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3db3b172-8eca-4127-e298-2f50c57a7580"
      },
      "source": [
        "y_pred  # What would be the best way to measure accuracy here?"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 1., ..., 0., 1., 0.],\n",
              "       [1., 0., 1., ..., 0., 1., 0.],\n",
              "       [1., 0., 1., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [1., 0., 1., ..., 0., 1., 0.],\n",
              "       [1., 0., 1., ..., 0., 1., 0.],\n",
              "       [1., 0., 1., ..., 0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). ✔\n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes) ✔\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network? ✔"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omxuIaqK8zVi",
        "colab_type": "text"
      },
      "source": [
        "### Strech Goal: Backpropagation (Examples, and high-level overview)\n",
        "\n",
        "Backpropagation is a way to update weights. There are different options, including SGD and other optimizers such as adam, nadam, and adamax that use different techniques to best optimize the effect of backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_zEMmov4TKf",
        "colab_type": "text"
      },
      "source": [
        "### Stretch Goal: Multi-Layered Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Kpr6Ec5ngn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining features / target\n",
        "\n",
        "features = df[['x1', 'x2', 'bias']].values\n",
        "target = df['y'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m8xLLrh4Sh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "e7ed63c2-feae-4cc9-d068-0ed39a496cd6"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Dense(3, activation='sigmoid', input_dim=features.shape[1], name='InputLayer'))\n",
        "model1.add(Dense(2, activation='sigmoid', name='HiddenLayerMLP'))\n",
        "model1.add(Dense(1, activation='sigmoid', name='OutputLayer'))\n",
        "\n",
        "model1.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "InputLayer (Dense)           (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "HiddenLayerMLP (Dense)       (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "OutputLayer (Dense)          (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 23\n",
            "Trainable params: 23\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ngBNbI6aeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "81780a85-db7a-4cdc-8785-3f1de44446fc"
      },
      "source": [
        "print(f'''Input values:\n",
        "{features}\n",
        "Shape: {features.shape}\n",
        "''')\n",
        "\n",
        "print(f'''Values to predict:\n",
        "{target}\n",
        "Shape: {target.shape}''')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input values:\n",
            "[[0 0 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [1 1 1]]\n",
            "Shape: (4, 3)\n",
            "\n",
            "Values to predict:\n",
            "[1 1 1 0]\n",
            "Shape: (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYjVjfb17LDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3318aa98-cca1-46ed-e0a8-277d84f41502"
      },
      "source": [
        "model1.fit(features, target, epochs=250)  # Let's fit our model :)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5858 - accuracy: 0.7500\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5855 - accuracy: 0.7500\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 947us/step - loss: 0.5852 - accuracy: 0.7500\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7500\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5846 - accuracy: 0.7500\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5843 - accuracy: 0.7500\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.7500\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.7500\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5835 - accuracy: 0.7500\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.7500\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.7500\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7500\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.7500\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7500\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.7500\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5815 - accuracy: 0.7500\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7500\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.7500\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5807 - accuracy: 0.7500\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.7500\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7500\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7500\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.7500\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7500\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.7500\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.7500\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7500\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.7500\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.7500\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.7500\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7500\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7500\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5772 - accuracy: 0.7500\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7500\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.7500\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7500\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.7500\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7500\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7500\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.7500\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7500\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.7500\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7500\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.7500\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5747 - accuracy: 0.7500\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.7500\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.7500\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5741 - accuracy: 0.7500\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.7500\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7500\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.7500\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7500\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.7500\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.7500\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5727 - accuracy: 0.7500\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.7500\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7500\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5722 - accuracy: 0.7500\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5720 - accuracy: 0.7500\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.7500\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7500\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7500\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7500\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7500\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7500\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.7500\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7500\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 0.7500\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7500\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7500\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.7500\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7500\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7500\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.7500\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7500\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5694 - accuracy: 0.7500\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.7500\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5691 - accuracy: 0.7500\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7500\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7500\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.7500\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5685 - accuracy: 0.7500\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7500\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7500\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.7500\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7500\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7500\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.7500\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.7500\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5675 - accuracy: 0.7500\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.7500\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.7500\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.7500\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7500\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7500\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.7500\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7500\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7500\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.7500\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7500\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.7500\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.7500\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7500\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7500\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.7500\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7500\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7500\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5655 - accuracy: 0.7500\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5654 - accuracy: 0.7500\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7500\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7500\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7500\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5650 - accuracy: 0.7500\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.7500\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7500\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7500\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7500\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.7500\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5645 - accuracy: 0.7500\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7500\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7500\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.7500\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.7500\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.7500\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7500\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7500\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7500\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7500\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7500\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7500\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7500\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.7500\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7500\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7500\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7500\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7500\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7500\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7500\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7500\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7500\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7500\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7500\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5626 - accuracy: 0.7500\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7500\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7500\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7500\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5623 - accuracy: 0.7500\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5623 - accuracy: 0.7500\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.7500\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.7500\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7500\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7500\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7500\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7500\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7500\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7500\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5617 - accuracy: 0.7500\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.7500\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.7500\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.7500\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7500\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7500\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7500\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7500\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5612 - accuracy: 0.7500\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7500\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7500\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7500\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7500\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7500\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5609 - accuracy: 0.7500\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.7500\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.7500\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5607 - accuracy: 0.7500\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7500\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7500\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7500\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.7500\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.7500\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.7500\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.7500\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5603 - accuracy: 0.7500\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5603 - accuracy: 0.7500\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7500\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7500\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7500\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7500\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7500\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7500\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7500\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7500\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7500\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7500\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7500\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7500\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7500\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7500\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7500\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7500\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7500\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7500\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7500\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7500\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7500\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7500\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7500\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7500\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7500\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7500\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5590 - accuracy: 0.7500\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5590 - accuracy: 0.7500\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7500\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7500\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7500\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7500\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7500\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7500\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7500\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7500\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7500\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7500\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7500\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7500\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7500\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7500\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7500\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7500\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7500\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7500\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7500\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7500\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5581 - accuracy: 0.7500\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.7500\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7500\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7500\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7500\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7500\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7500\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.7500\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7500\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7500\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7500\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7500\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7500\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7500\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7500\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7500\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7500\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7500\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5574 - accuracy: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6755bbcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3wvTcFG9Vir",
        "colab_type": "text"
      },
      "source": [
        "### Stretch Goal: Activation Functions\n",
        "\n",
        "Just like anything else, usage of which activation function is determined by the problem itself. One way to optimize your activation function is with parameter tuning using GridSearchCV or RandomizedSearchCV. However, there are problems such as predicting probability between 0 and 1, where sigmoid functions perform best. Again, this is use-case specific."
      ]
    }
  ]
}